{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import mlflow\n",
    "import parfit.parfit as pf\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "12500 12500\n",
      "(25000, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "directory='train'\n",
    "images=[]\n",
    "labels=[]\n",
    "i=0\n",
    "for i,image in enumerate(os.listdir(directory)):\n",
    "    #if i<100 or i>24900:\n",
    "    if (i%1000==0):\n",
    "        print(i)\n",
    "    if image[0]=='c':\n",
    "        labels.append(0)\n",
    "    else: \n",
    "        labels.append(1)\n",
    "    an_image = PIL.Image.open(directory + '/' + image)\n",
    "    new_image = an_image.resize((200, 200))\n",
    "    #new_image = an_image.resize((100, 100))\n",
    "    image_array = np.array(new_image)\n",
    "    image_array=np.squeeze(image_array)\n",
    "    #if (i==1):\n",
    "    #    print(image_array)\n",
    "    images.append(image_array)\n",
    "k=0\n",
    "for i in labels:\n",
    "    k+=i\n",
    "print(k,len(labels)-k)\n",
    "images=np.array(images)\n",
    "labels=np.array(labels)\n",
    "\n",
    "#print(labels)\n",
    "#print(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hog():\n",
    "    winSize = (200, 200)\n",
    "    blockSize = (100, 100)\n",
    "    blockStride = (20, 20)\n",
    "    cellSize = (50, 50)\n",
    "   # winSize = (100, 100)\n",
    "   # blockSize = (50, 50)\n",
    "   # blockStride = (10, 10)\n",
    "   # cellSize = (25, 25)\n",
    "    nbins = 9\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "    signedGradient = True\n",
    "\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins, derivAperture, winSigma,\n",
    "                            histogramNormType, L2HysThreshold, gammaCorrection, nlevels, signedGradient)\n",
    "\n",
    "    return hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18469623 0.15193282 0.06745876 ... 0.14829217 0.18786134 0.17437679]\n"
     ]
    }
   ],
   "source": [
    "hog = get_hog()\n",
    "rand = np.random.RandomState(10)\n",
    "shuffle=rand.permutation(len(images))\n",
    "#print(images)\n",
    "images=np.array(images)\n",
    "labels=np.array(labels)\n",
    "images=images[shuffle]\n",
    "labels=labels[shuffle]\n",
    "\n",
    "#print(images)\n",
    "hog_descriptors=[]\n",
    "for image in images:\n",
    "    hog_descriptors.append(hog.compute(image))\n",
    "hog_descriptors=np.squeeze(hog_descriptors)\n",
    "hog_descriptors=np.array(hog_descriptors)\n",
    "print(hog_descriptors[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 200, 200, 3) (2500, 200, 200, 3) (22500, 1296) (2500, 1296)\n"
     ]
    }
   ],
   "source": [
    "train_n=int(0.9*len(images))\n",
    "train_labels,test_labels = np.split(labels,[train_n])\n",
    "train_images, test_images = np.split(images,[train_n])\n",
    "train_hog_descriptors, test_hog_descriptors = np.split(hog_descriptors,[train_n])\n",
    "print(train_images.shape,test_images.shape,train_hog_descriptors.shape, test_hog_descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "accuracy:  0.702\n",
      "[[360 166]\n",
      " [132 342]]\n"
     ]
    }
   ],
   "source": [
    "params  = {'C': 5, 'coef0': 0.003, 'degree': 3, 'gamma': 0.00005, 'kernel': 'sigmoid'}\n",
    "mlflow.set_experiment('agressive')\n",
    "with mlflow.start_run():\n",
    "    train_labels_part = train_labels[:1000]\n",
    "    test_labels_part = test_labels[:1000]\n",
    "    train_hog_descriptors_part = train_hog_descriptors[:1000]\n",
    "    test_hog_descriptors_part = test_hog_descriptors[:1000]\n",
    "    print(len(train_labels_part))\n",
    "    model=SVC(**params)\n",
    "    model.fit(train_hog_descriptors_part, train_labels_part)\n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    clf.fit(train_hog_descriptors_part, train_labels_part)\n",
    "    predictions = clf.predict(test_hog_descriptors_part)\n",
    "    mlflow.log_param('params', params)\n",
    "    mlflow.log_metric('f1', metrics.f1_score(predictions, test_labels_part, average = 'weighted'))\n",
    "    print('accuracy: ', accuracy_score(predictions, test_labels_part))\n",
    "    print(confusion_matrix(test_labels_part, clf.predict(test_hog_descriptors_part)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'C' : np.arange(0.1,1,0.1),\n",
    "    'degree': np.arange(3,10,1),\n",
    "    'coef0': np.arange(0.001,5,0.5),\n",
    "    'gamma': ('auto', 'scale')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(0.0, 1.0, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSmodel=SVC()\n",
    "#GridS = GridSearchCV(GSmodel,params, cv=5,verbose=3)\n",
    "#GridS.fit(train_hog_descriptors_part, train_labels_part)\n",
    "#print('accuracy: ', accuracy_score(clf.predict(test_hog_descriptors_part), test_labels_part))\n",
    "#print(confusion_matrix(test_labels_part, clf.predict(test_hog_descriptors_part)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('accuracy: ', accuracy_score(GridS.predict(test_hog_descriptors_part), test_labels_part))\n",
    "#print(confusion_matrix(test_labels_part, clf.predict(test_hog_descriptors_part)))\n",
    "#print('Best params ',GridS.best_params_)\n",
    "nice_params1={'C': 1, 'coef0': 0.001, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "nice_params2={'C': 5, 'coef0': 0.001, 'degree': 3, 'gamma': 0.0006, 'kernel': 'rbf'}\n",
    "nice_params3={'C': 5, 'coef0': 6, 'degree': 10, 'gamma': 0.001, 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=SVC(**nice_params1)\n",
    "model1.fit(train_hog_descriptors, train_labels)\n",
    "clf1 = make_pipeline(StandardScaler(), model1)\n",
    "clf1.fit(train_hog_descriptors, train_labels)\n",
    "print('First accuracy: ', accuracy_score(clf1.predict(test_hog_descriptors),test_labels))\n",
    "\n",
    "model2=SVC(**nice_params2)\n",
    "model2.fit(train_hog_descriptors, train_labels)\n",
    "clf2 = make_pipeline(StandardScaler(), model2)\n",
    "clf2.fit(train_hog_descriptors, train_labels)\n",
    "print('Second accuracy: ', accuracy_score(clf2.predict(test_hog_descriptors),test_labels))\n",
    "\n",
    "#model3=SVC(**nice_params3)\n",
    "#model3.fit(train_hog_descriptors, train_labels)\n",
    "#clf3 = make_pipeline(StandardScaler(), model3)\n",
    "#clf3.fit(train_hog_descriptors, train_labels)\n",
    "#print('Third accuracy: ', accuracy_score(clf3.predict(test_hog_descriptors),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(best_model.predict(test_hog_descriptors),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = cross_val_score(best_model, train_hog_descriptors, train_labels, scoring='accuracy', cv = 10)\n",
    "#print('cross val accuracy: ', accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_params={'C': 1, 'coef0': 0.001, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "train_labels_part = train_labels[:5000]\n",
    "test_labels_part = test_labels[:1000]\n",
    "train_hog_descriptors_part = train_hog_descriptors[:5000]\n",
    "test_hog_descriptors_part = test_hog_descriptors[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter  =  1  accuracy:  0.6684 f1:  0.6606631191158412\n",
      "max_iter  =  10  accuracy:  0.6776 f1:  0.6715566422167889\n",
      "max_iter  =  100  accuracy:  0.6624 f1:  0.6456759026028547\n",
      "max_iter  =  1000  accuracy:  0.6848 f1:  0.6799350121852153\n",
      "max_iter  =  10000  accuracy:  0.6764 f1:  0.678585617798967\n",
      "max_iter  =  1000000  accuracy:  0.7188 f1:  0.7157298827335221\n"
     ]
    }
   ],
   "source": [
    "x=[1, 10, 100, 1000, 10000, 1000000]\n",
    "for i in x:\n",
    "    clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=i, tol=1e-3))\n",
    "    clf.fit(train_hog_descriptors, train_labels)\n",
    "    print('max_iter', ' = ', i, ' accuracy: ', accuracy_score(clf.predict(test_hog_descriptors), test_labels),\n",
    "        'f1: ', f1_score(clf.predict(test_hog_descriptors), test_labels))\n",
    "    #print(clf.predict(test_hog_descriptors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "for i in x:\n",
    "    clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=200, tol=i))\n",
    "    clf.fit(train_hog_descriptors, train_labels)\n",
    "    print('tol', ' = ', i, ' accuracy: ', accuracy_score(clf.predict(test_hog_descriptors), test_labels),\n",
    "        'f1: ', f1_score(clf.predict(test_hog_descriptors), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------FITTING MODELS-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 924 out of 924 | elapsed: 14.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------SCORING MODELS-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 728 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 924 out of 924 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many dimensions to plot.\n",
      "SGDClassifier(alpha=0.0004, l1_ratio=0.12, loss='modified_huber', n_jobs=-1) 0.8361776776930644\n"
     ]
    }
   ],
   "source": [
    "grid = {\n",
    "    'alpha': [1e-5, 1e-4, 2*1e-4, 4*1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6], \n",
    "    'max_iter': [1000],\n",
    "    'loss': ['log'], \n",
    "    'l1_ratio':[0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.13,0.14,0.15,0.2],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'n_jobs': [-1],\n",
    "    'loss': ['modified_huber','log','hinge']\n",
    "}\n",
    "paramGrid = ParameterGrid(grid)\n",
    "\n",
    "bestModel, bestScore, allModels, allScores = pf.bestFit(SGDClassifier, paramGrid,\n",
    "           train_hog_descriptors, train_labels, test_hog_descriptors, test_labels, \n",
    "           metric = roc_auc_score,\n",
    "           scoreLabel = \"accuracy\")\n",
    "\n",
    "print(bestModel, bestScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=0.001))\n",
    "#sgdc_gs = GridSearchCV(clf, sgdc_params, cv=5, verbose=1, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function bestFit in module parfit.parfit:\n",
      "\n",
      "bestFit(model, paramGrid, X_train, y_train, X_val=None, y_val=None, nfolds=5, metric=<function roc_auc_score at 0x0000025EA82BD160>, greater_is_better=True, predict_proba=True, showPlot=True, scoreLabel=None, vrange=None, cmap='YlOrRd', n_jobs=-1, verbose=10)\n",
      "    Parallelizes choosing the best fitting model on the validation set, doing a grid search over the parameter space.\n",
      "        Models are scored using specified metric, and user must determine whether the best score is the 'max' or 'min' of scores.\n",
      "    :param model: The instantiated model you wish to pass, e.g. LogisticRegression()\n",
      "    :param paramGrid: The ParameterGrid object created from sklearn.model_selection\n",
      "    :param X_train: The independent variable data used to fit the models\n",
      "    :param y_train: The dependent variable data used to fit the models\n",
      "    :param X_val: The independent variable data used to score the models\n",
      "    :param y_val: The dependent variable data used to score the models\n",
      "    :param nfolds: Cross-validation number of folds, used if a validation set is not specified.\n",
      "        Set nfolds=X.shape[0] for LeaveOneOut cross-validation.\n",
      "    :param metric: The metric used to score the models, e.g. imported from sklearn.metrics\n",
      "    :param greater_is_better: Choice between optimizing for greater scores or lesser scores\n",
      "        Default True means greater and False means lesser\n",
      "    :param predict_proba: Choice between 'predict_proba' and 'predict' for scoring routine\n",
      "        Default True means predict_proba and False means predict\n",
      "    :param showPlot: Whether or not to display the plot of the scores over the parameter grid\n",
      "    :param scoreLabel: The specified label (dependent on scoring metric used), e.g. 'AUC'\n",
      "    :param vrange: The visible range over which to display the scores\n",
      "    :param cmap: The chosen colormap for 2D and 3D plotting. Default is YlOrRd.\n",
      "        You can invert your chosen colormap by adding '_r' to the end\n",
      "    :param n_jobs: Number of cores to use in parallelization (defaults to -1: all cores)\n",
      "    :param verbose: The level of verbosity of reporting updates on parallel process\n",
      "        Default is 10 (send an update at the completion of each job)\n",
      "    :return: Returns a tuple including the best scoring model, the score of the best model, all models, and all scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pf.bestFit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy:  0.7584 f1:  0.7516447368421053\n",
      "cross val accuracy:  0.7476444444444444\n"
     ]
    }
   ],
   "source": [
    "print(' accuracy: ', accuracy_score(bestModel.predict(test_hog_descriptors), test_labels),\n",
    "        'f1: ', f1_score(bestModel.predict(test_hog_descriptors), test_labels))\n",
    "accuracy = cross_val_score(bestModel, train_hog_descriptors, train_labels, scoring='accuracy', cv = 10)\n",
    "print('cross val accuracy: ', accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(cтратифицированый сплит)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First accuracy:  0.8102\n",
      "accuracy:  0.7977714285714286\n"
     ]
    }
   ],
   "source": [
    "nice_params1={'C': 1, 'coef0': 0.001, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
    "nice_params2={'C': 5, 'coef0': 0.001, 'degree': 3, 'gamma': 0.0006, 'kernel': 'rbf'}\n",
    "\n",
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.4, random_state=1, stratify=labels)\n",
    "model1=SVC(**nice_params1)\n",
    "model1.fit(train_hog_descriptors, train_labels)\n",
    "clf1 = make_pipeline(StandardScaler(), model1)\n",
    "clf1.fit(train_hog_descriptors, train_labels)\n",
    "print('First accuracy: ', accuracy_score(clf1.predict(test_hog_descriptors),test_labels))\n",
    "\n",
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.7, random_state=1, stratify=labels)\n",
    "model2=SVC(**nice_params2)\n",
    "model2.fit(train_hog_descriptors, train_labels)\n",
    "clf2 = make_pipeline(StandardScaler(), model2)\n",
    "clf2.fit(train_hog_descriptors, train_labels)\n",
    "print('accuracy: ', accuracy_score(clf2.predict(test_hog_descriptors),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First accuracy:  0.7954857142857142\n",
      "accuracy:  0.8221\n"
     ]
    }
   ],
   "source": [
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.7, random_state=1, stratify=labels)\n",
    "model1=SVC(**nice_params1)\n",
    "model1.fit(train_hog_descriptors, train_labels)\n",
    "clf1 = make_pipeline(StandardScaler(), model1)\n",
    "clf1.fit(train_hog_descriptors, train_labels)\n",
    "print('First accuracy: ', accuracy_score(clf1.predict(test_hog_descriptors),test_labels))\n",
    "\n",
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.4, random_state=1, stratify=labels)\n",
    "model2=SVC(**nice_params2)\n",
    "model2.fit(train_hog_descriptors, train_labels)\n",
    "clf2 = make_pipeline(StandardScaler(), model2)\n",
    "clf2.fit(train_hog_descriptors, train_labels)\n",
    "print('accuracy: ', accuracy_score(clf2.predict(test_hog_descriptors),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size = 0.2, accuracy:  0.8202\n",
      "train_size = 0.2, accuracy:  0.83\n"
     ]
    }
   ],
   "source": [
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.2, random_state=1, stratify=labels)\n",
    "model1=SVC(**nice_params1)\n",
    "model1.fit(train_hog_descriptors, train_labels)\n",
    "clf1 = make_pipeline(StandardScaler(), model1)\n",
    "clf1.fit(train_hog_descriptors, train_labels)\n",
    "print('train_size = 0.2, accuracy: ', accuracy_score(clf1.predict(test_hog_descriptors),test_labels))\n",
    "\n",
    "train_hog_descriptors, test_hog_descriptors, train_labels, test_labels = train_test_split(hog_descriptors, labels,test_size=0.2, random_state=1, stratify=labels)\n",
    "model2=SVC(**nice_params2)\n",
    "model2.fit(train_hog_descriptors, train_labels)\n",
    "clf2 = make_pipeline(StandardScaler(), model2)\n",
    "clf2.fit(train_hog_descriptors, train_labels)\n",
    "print('train_size = 0.2, accuracy: ', accuracy_score(clf2.predict(test_hog_descriptors),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
